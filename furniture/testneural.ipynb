{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from skimage import transform\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "#mlb=MultiLabelBinarizer()\n",
    "\n",
    "# # read data\n",
    "def load_images_from_folder(folder):\n",
    "    X = []\n",
    "    y = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = io.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            label = re.findall(r'_([^.]*).',filename)\n",
    "            X.append(img)\n",
    "            y.append(int(label[0]))\n",
    "    return X,y\n",
    "\n",
    "train = np.array(load_images_from_folder('train937'))\n",
    "X_train = train[0]\n",
    "y_train = train[1]\n",
    "y_train=y_train.astype('int')\n",
    "\n",
    "test = np.array(load_images_from_folder('valid516'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "X_train_resized = [transform.resize(img,(50,50,3)) for img in X_train]\n",
    "\n",
    "X_train_flat = np.array([img.flatten() for img in X_train_resized])\n",
    "\n",
    "X_test = test[0]\n",
    "y_test = test[1]\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "\n",
    "X_test_resized = [transform.resize(img,(50,50,3)) for img in X_test]\n",
    "X_test_flat = np.array([img.flatten() for img in X_test_resized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca=IncrementalPCA(n_components=300, batch_size=None,whiten=True)\n",
    "n_components = ipca.n_components\n",
    "ipca.fit(X_train_flat)\n",
    "X_train_flat=ipca.transform(X_train_flat)\n",
    "X_test_flat=ipca.transform(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n",
      "(516,)\n"
     ]
    }
   ],
   "source": [
    "x_train=X_train_flat\n",
    "x_test=X_test_flat\n",
    "#y_train=y_train-1\n",
    "#y_test=y_test-1\n",
    "\n",
    "def one_hot_enc(label, n_labels):\n",
    "    one_hot = np.zeros(n_labels)\n",
    "    one_hot[label-1] = 1\n",
    "    return one_hot\n",
    "\n",
    "#all_labels=[]\n",
    "#for i in range(1,129):\n",
    "#    all_labels.append(i)\n",
    "#mlb.fit(all_labels)\n",
    "#y_train=mlb.transform(y_train)\n",
    "print(y_train.shape)\n",
    "#y_test=mlb.transform(y_test)\n",
    "print(y_test.shape)\n",
    "y_train = np.array([one_hot_enc(label, 128) for label in y_train])\n",
    "y_test = np.array([one_hot_enc(label, 128) for label in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "[[2 3]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0,0,1,0])\n",
    "b = np.array([0,0,1,0])\n",
    "print(np.array_equal(a, b))\n",
    "\n",
    "print(np.where(a==1)[0][0])\n",
    "                  \n",
    "c = np.array([[1,2], [3,4]])\n",
    "c = c + 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "EPOCH 500\n",
      "EPOCH 1000\n",
      "EPOCH 1500\n",
      "EPOCH 2000\n"
     ]
    }
   ],
   "source": [
    "### Tensorflow Neural Network\n",
    "# set number of neurons in hidden layer M\n",
    "M = 300\n",
    "\n",
    "# Initialize placeholders\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, n_components])\n",
    "y = tf.placeholder(dtype = tf.int32, shape = [None,128])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([M, M], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([M]), name='b1')\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([M, 128], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([128]), name='b2')\n",
    "\n",
    "# calculate the output of the hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)\n",
    "\n",
    "logits = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))\n",
    "\n",
    "# maybe add some clipped shit here\n",
    "\n",
    "# Define a loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y,\n",
    "                                                                    logits = logits))\n",
    "# Define an optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "# Convert logits to label indexes\n",
    "correct_pred = tf.argmax(logits, 1)\n",
    "\n",
    "# Define an accuracy metric\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(2001):\n",
    "        _, accuracy_val = sess.run([train_op, accuracy], feed_dict={x: x_train, y: y_train})\n",
    "        if i % 500 == 0:\n",
    "            print('EPOCH', i)\n",
    "            #print(\"Loss: \", loss)\n",
    "        #print('DONE WITH EPOCH')\n",
    "            #print(accuracy_val)\n",
    "\n",
    "predicted = sess.run([correct_pred], feed_dict={x: x_test})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"/tmp/tb/testneutral\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_dec(one_hot_arr):\n",
    "    index = np.where(one_hot_arr==1)[0][0]\n",
    "    label = index+1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "predicted\n",
      "[ 80  25 112  71 117  51 121 112  25  67  25 106  32  80  15  30  36  68\n",
      " 110  91  25  63  25  56 106  75  98 116 112   4  32  87  35  70  32 106\n",
      " 126  78  32  43   6  59  69  48 117 126  35  32  43  80  92  97 106  35\n",
      "  31  93  69  63  25  80  59  59 112  75 108  99  93  80  31  32  10   3\n",
      "  73  93  94  68 112  69  56  30  68  27  83  51  32  32  25  78  19  56\n",
      "  72  25  32  32  51  31  68  71  31  80  53  69 112   4  59  87  48 111\n",
      "  10 106  13 115   3  43  83  39  27  68 118  35 117  68  98  25  77  32\n",
      " 106   2  20  10  22  36  93  36  93  68  43   6  43  25 127  68  43  22\n",
      "  20  32  79  48  36  93  95   5 127  67  86  32  75  71  71  80  20 117\n",
      "  30  32 124  63 113  27   1  98  36  43  30  68  62  87  62 110   9  22\n",
      " 122  60  25 112  97  75 106   9   6  79  93  54  98  48  59  32  83 106\n",
      "  79  98  25  91 122  88 108  69  80  32  32 106 106 121  63  73  19  21\n",
      "  71  90 124 106   3 112  48  32  32  77 106  97   3  10  77  78 119  32\n",
      " 106  67   3  32 117  68  43  32  38 126  51  91  36  25  25  59  19  32\n",
      "  62  96  32  54  68  36  59  88 119  68  32  87  32 115  10 106 106  41\n",
      "  96 126  69  23  56  32  21  32  91 124 106  80  99 112 112  32  53  11\n",
      "  59  68  44  31  91  16  54  68  19  77  55  70   2 112  48 106  59  19\n",
      "  19 111  27 108  56  60  32  19  21 106 102  32  83 106  92 119  43  30\n",
      "  77 119  25  75  68  38  32  92  91  71  17  32  90  59 106  32 108  41\n",
      "  68  20 106  36  32  73  22  71  80  22 106   3  27  80  92 117  70  21\n",
      "  71  25  27  31  36  69  32  37  71  41  51 119 124  21 124  68  99  31\n",
      "  51  43  38  78  35  12  91  59  42  92 106  31  32  10  25   3   0 127\n",
      "  75  32  72 121  11   2  90  72  78  53  96  93 115 106 106  68   4  32\n",
      " 119  30  94  36  98  97  68   3 113  80 106 116 106  92  51  27 126  56\n",
      "  91 108  68  92   3  48  90  98  57  53  51  25  78 121  60  91  25  32\n",
      " 112  94 106  85  32  32  43  67  28  69  38 117 112  31  78 121 112 106\n",
      " 117  51  32  78  68  93  32  75 124 112  87  93   5  68  43  32 106  32\n",
      "  32  32  71 106  68 124  97  96  36 119  11   9  48  93  48  41 124  68\n",
      "  78 119  38  15  73  70  96  25  37   4 114  97]\n",
      "y_test\n",
      "[ 40  91  59 113  56  41  96   3  46  76  71  75  54  86  77  94  90  22\n",
      "  58  87  62 113 128  77  98  59   6 121   5 125  90  10  55  62 123  97\n",
      "  89   1 124 119 124   8  63 120  63  96  91 118  89 118 100  80  70  12\n",
      " 104  42  39 126  54  38 108 125  27  27  96  25   8  84   4   6  56  91\n",
      "  66  28 108 111 128 118  22  61  66  80  35 105 115 100  22  93  57 103\n",
      "  73  88  92  93  15  36  61  62  14  37  50  95  52  60 107  16 107  92\n",
      "  44 120  14  86  52   3 128  71  36  70  17  21  16  60 124  44  78  49\n",
      " 122  58  39   2  32  45  51 113  58  41 123 123  89 119 122  29 104  58\n",
      "  93 101  71 126  36  47  38  19  18  24   8 123  41  85  96  53  45  99\n",
      "  82  65  19 107  61  28 120  67  34 125  38  94  16  47  55  74  25  26\n",
      " 127  49  54  91   9  31  65  28  73 113  32  33  21  53  89  56  54  86\n",
      "  80  61 100 100  29  79  13 127  51 127 119  87  24   8  59 127  79   2\n",
      " 107  60  62  85  96   1  59  33  99  42 126  11  41 125  41  85  91  19\n",
      " 125  96 121  33   1  25  30  79 109   2  38 102  62  51  43  81   1 124\n",
      "  63  55 127  80   1 102  35  63 127 125  66  28 112  70  68   8  15  82\n",
      "  70  32  71  99 127 101  86  69 104 123  85  43  19  10  46  98  88 104\n",
      "  19  21  74  50 105  17 126  55  16 111  25 110  58  33  99 107   3  29\n",
      "   1  95   7   3 128  89  71  66 119  99   4  33  90  67 113  80  35  37\n",
      "  78  13 108  19  69  63  69 117  19 118 107  98 115  58  48  71  51  61\n",
      " 103  23   5  29 116   4 128  69  44  80  27  16 115 121 108  79 102  24\n",
      "  91 116  75  19  91   3 118  97 115   8 121  92  68  61   1  39  26  50\n",
      "  27  73  39  53  72  31  96  70  71  88  80  27  38  50 124  30 117  20\n",
      "  91 116  73 112  89  58  79  78 117  98  97 128  32 114 101  20  35  76\n",
      " 106  75  95  27  35  22  85   1  13  36  32 121  48  21   7  53  94  64\n",
      "  93  95  97 123  16 110 128  90  58 127   5  57  97  31  13  52  45  49\n",
      " 107  60  25  26 121  12  51  33  70 106  87 111  96  86 108  75  59  63\n",
      "  67  44 125 120  59  21  56  55  27  53  50  12  59   2  47  15   7  50\n",
      "  84  54 112  48  10  37   1  28  86  17  57  23  73 128  12  61 107 108\n",
      "   1  98   1  55  15  27 112  99 126  65  26  37]\n",
      "predicted\n",
      "[ 81  26 113  72 118  52 122 113  26  68  26 107  33  81  16  31  37  69\n",
      " 111  92  26  64  26  57 107  76  99 117 113   5  33  88  36  71  33 107\n",
      " 127  79  33  44   7  60  70  49 118 127  36  33  44  81  93  98 107  36\n",
      "  32  94  70  64  26  81  60  60 113  76 109 100  94  81  32  33  11   4\n",
      "  74  94  95  69 113  70  57  31  69  28  84  52  33  33  26  79  20  57\n",
      "  73  26  33  33  52  32  69  72  32  81  54  70 113   5  60  88  49 112\n",
      "  11 107  14 116   4  44  84  40  28  69 119  36 118  69  99  26  78  33\n",
      " 107   3  21  11  23  37  94  37  94  69  44   7  44  26 128  69  44  23\n",
      "  21  33  80  49  37  94  96   6 128  68  87  33  76  72  72  81  21 118\n",
      "  31  33 125  64 114  28   2  99  37  44  31  69  63  88  63 111  10  23\n",
      " 123  61  26 113  98  76 107  10   7  80  94  55  99  49  60  33  84 107\n",
      "  80  99  26  92 123  89 109  70  81  33  33 107 107 122  64  74  20  22\n",
      "  72  91 125 107   4 113  49  33  33  78 107  98   4  11  78  79 120  33\n",
      " 107  68   4  33 118  69  44  33  39 127  52  92  37  26  26  60  20  33\n",
      "  63  97  33  55  69  37  60  89 120  69  33  88  33 116  11 107 107  42\n",
      "  97 127  70  24  57  33  22  33  92 125 107  81 100 113 113  33  54  12\n",
      "  60  69  45  32  92  17  55  69  20  78  56  71   3 113  49 107  60  20\n",
      "  20 112  28 109  57  61  33  20  22 107 103  33  84 107  93 120  44  31\n",
      "  78 120  26  76  69  39  33  93  92  72  18  33  91  60 107  33 109  42\n",
      "  69  21 107  37  33  74  23  72  81  23 107   4  28  81  93 118  71  22\n",
      "  72  26  28  32  37  70  33  38  72  42  52 120 125  22 125  69 100  32\n",
      "  52  44  39  79  36  13  92  60  43  93 107  32  33  11  26   4   1 128\n",
      "  76  33  73 122  12   3  91  73  79  54  97  94 116 107 107  69   5  33\n",
      " 120  31  95  37  99  98  69   4 114  81 107 117 107  93  52  28 127  57\n",
      "  92 109  69  93   4  49  91  99  58  54  52  26  79 122  61  92  26  33\n",
      " 113  95 107  86  33  33  44  68  29  70  39 118 113  32  79 122 113 107\n",
      " 118  52  33  79  69  94  33  76 125 113  88  94   6  69  44  33 107  33\n",
      "  33  33  72 107  69 125  98  97  37 120  12  10  49  94  49  42 125  69\n",
      "  79 120  39  16  74  71  97  26  38   5 115  98]\n",
      "Accuracy: 0.035\n"
     ]
    }
   ],
   "source": [
    "# 0-1 loss is based on match count\n",
    "print(\"y_test\")\n",
    "print(y_test)\n",
    "print(\"predicted\")\n",
    "print(predicted)\n",
    "\n",
    "# y_test is an array of one-hot arrays, while y_pred is an array of labels, thus we decode y_test\n",
    "y_test = np.array([one_hot_dec(one_hot) for one_hot in y_test])\n",
    "\n",
    "# the Neural network is written to output with 0-offset so we need to adjust \"predicted\"\n",
    "predicted = predicted + 1\n",
    "\n",
    "print(\"y_test\")\n",
    "print(y_test)\n",
    "print(\"predicted\")\n",
    "print(predicted)\n",
    "\n",
    "match_count = sum([int(np.array_equal(y_true, y_pred)) for y_true, y_pred in zip(y_test, predicted)])\n",
    "# Calculate the accuracy\n",
    "accuracy = match_count / len(y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
